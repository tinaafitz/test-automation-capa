---
# handlers file for test-autonode-feature
# Handlers are triggered by notify directives in tasks

- name: verify karpenter pods
  shell: |
    kubectl --kubeconfig={{ autonode_kubeconfig_path }} get pods -n {{ karpenter_namespace }} -l {{ karpenter_label_selector }}
  register: karpenter_pods_status
  changed_when: false
  failed_when: false

- name: wait for karpenter ready
  shell: |
    kubectl --kubeconfig={{ autonode_kubeconfig_path }} wait --for=condition=ready pod -n {{ karpenter_namespace }} -l {{ karpenter_label_selector }} --timeout=60s
  register: karpenter_wait_result
  changed_when: false
  failed_when: false

- name: refresh nodepool status
  shell: |
    kubectl --kubeconfig={{ autonode_kubeconfig_path }} get nodepools
  register: nodepools_status
  changed_when: false
  failed_when: false

- name: refresh ec2nodeclass status
  shell: |
    kubectl --kubeconfig={{ autonode_kubeconfig_path }} get ec2nodeclass
  register: ec2nodeclass_status
  changed_when: false
  failed_when: false

- name: check rosacontrolplane reconciliation
  shell: |
    oc get rosacontrolplane {{ cluster_name }} -n {{ cluster_namespace }} -o jsonpath='{.status.ready}'
  register: rosacontrolplane_ready
  changed_when: false
  failed_when: false

- name: get karpenter logs
  shell: |
    kubectl --kubeconfig={{ autonode_kubeconfig_path }} logs -n {{ karpenter_namespace }} -l {{ karpenter_label_selector }} --tail=50
  register: karpenter_recent_logs
  changed_when: false
  failed_when: false

- name: check cluster events
  shell: |
    kubectl --kubeconfig={{ autonode_kubeconfig_path }} get events --all-namespaces --sort-by='.lastTimestamp' | tail -20
  register: recent_cluster_events
  changed_when: false
  failed_when: false

- name: verify aws resource tags
  shell: |
    aws ec2 describe-security-groups --filters "Name=tag:{{ tag_keys.karpenter_discovery }},Values={{ cluster_id }}" --query 'SecurityGroups[*].[GroupId,GroupName]' --output table
  environment:
    AWS_DEFAULT_REGION: "{{ aws_region }}"
  register: tagged_security_groups
  changed_when: false
  failed_when: false

- name: verify aws subnets tags
  shell: |
    aws ec2 describe-subnets --filters "Name=tag:{{ tag_keys.karpenter_discovery }},Values={{ cluster_id }}" --query 'Subnets[*].[SubnetId,AvailabilityZone]' --output table
  environment:
    AWS_DEFAULT_REGION: "{{ aws_region }}"
  register: tagged_subnets
  changed_when: false
  failed_when: false

- name: refresh iam role status
  shell: |
    aws iam get-role --role-name {{ autonode_role_name }} --query 'Role.[RoleName,Arn]' --output text
  environment:
    AWS_DEFAULT_REGION: "{{ aws_region }}"
  register: iam_role_status
  changed_when: false
  failed_when: false

- name: refresh iam policy status
  shell: |
    aws iam list-policies --scope Local --query "Policies[?PolicyName=='{{ autonode_policy_name }}'].[PolicyName,Arn]" --output text
  environment:
    AWS_DEFAULT_REGION: "{{ aws_region }}"
  register: iam_policy_status
  changed_when: false
  failed_when: false

- name: save test results
  copy:
    content: |
      # AutoNode Test Results - {{ ansible_date_time.iso8601 }}
      Cluster: {{ cluster_name }}
      Region: {{ aws_region }}
      AutoNode Mode: {{ autonode_mode }}

      ## Handler Execution Summary
      This file was generated by an Ansible handler execution.
    dest: "{{ autonode_results_dir }}/handler-results-{{ ansible_date_time.epoch }}.txt"
    mode: '0644'
  when: autonode_results_dir is defined
  changed_when: false
  failed_when: false

- name: display notification
  debug:
    msg: |
      ========================================
      Handler Notification
      ========================================
      A change was detected that triggered this handler.
      Cluster: {{ cluster_name }}
      Time: {{ ansible_date_time.iso8601 }}
      ========================================
  changed_when: false
